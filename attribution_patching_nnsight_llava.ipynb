{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seiji-Armstrong/vlm-interp/blob/main/attribution_patching_nnsight_llava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkHnFWlv-b43"
      },
      "source": [
        "# Setup\n",
        "\n",
        "copy pasted from https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V31Nj-SF-bQT",
        "outputId": "0953da16-8003-4aef-f15b-cfd058ce628a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEBUG_MODE = False\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKFzAtEt-gfO"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEBUG_MODE:\n",
        "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z-KknxI_BRpd"
      },
      "outputs": [],
      "source": [
        "!pip install nnsight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "N9YQB565cb7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IEIkMeJ73_1t"
      },
      "outputs": [],
      "source": [
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "import torch\n",
        "import einops\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "if IN_COLAB:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "else:\n",
        "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "                                               cache_dir=\"./checkpoints\")\n",
        "\n",
        "llava = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "                                                          cache_dir=\"./checkpoints\",\n",
        "                                                          torch_dtype=torch.float16,\n",
        "                                                          low_cpu_mem_usage=True)\n",
        "llava.to(device)\n",
        "\n",
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(llava)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount and Load images from Google Drive"
      ],
      "metadata": {
        "id": "eavp8H0QxeIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqTxPBVNCip7",
        "outputId": "f5d966ba-cb39-4112-c8fd-f6aaa12e483a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgCZcuirCeV5"
      },
      "outputs": [],
      "source": [
        "img_dir = '/content/gdrive/MyDrive/vlm_mats_images/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HedWGeQEPOsy"
      },
      "source": [
        "# Playground for new conflict pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAyAy7K9Cz73"
      },
      "outputs": [],
      "source": [
        "# square basketball\n",
        "img_path = '/content/gdrive/MyDrive/vlm_mats_images/square_basketball.webp'\n",
        "img_square = Image.open(img_path)\n",
        "img_square = img_square.resize((1024, 1024))\n",
        "img_square_small = img_square.resize((300, 300))\n",
        "\n",
        "\n",
        "# triangle basketball\n",
        "img_path = '/content/gdrive/MyDrive/vlm_mats_images/triangle_basketball_2.webp'\n",
        "img_tri = Image.open(img_path)\n",
        "img_tri = img_tri.resize((1024, 1024))\n",
        "img_tri_small = img_tri.resize((300, 300))\n",
        "\n",
        "# round basketball\n",
        "img_path = '/content/gdrive/MyDrive/vlm_mats_images/round_basketball.jpeg'\n",
        "img_round = Image.open(img_path)\n",
        "img_round = img_round.resize((1024, 1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2VENOOXNe9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950746ee-3234-411d-9c30-39de6657d666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]  \n",
            "Is the object in the image a round basketball? Answer Yes or No.[/INST] Yes, the object in the image is a round basketball. \n",
            "\n",
            "\n",
            "[INST]  \n",
            "Is the object in the image a round basketball? Answer Yes or No.[/INST] No, the object in the image is not a round basketball. It is a three-dimensional rendering or sculpture of a basketball, which is not a real object but rather a representation of one. \n"
          ]
        }
      ],
      "source": [
        "prompt = f\"[INST] <image>\\nIs the object in the image a round basketball? Answer Yes or No.[/INST]\"\n",
        "inputs_square = processor(images=img_square, text=prompt, return_tensors=\"pt\").to(device)\n",
        "inputs_tri = processor(images=img_tri, text=prompt, return_tensors=\"pt\").to(device)\n",
        "llava_square_out = llava.generate(**inputs_square, max_new_tokens=100)\n",
        "res_square = processor.decode(llava_square_out[0], skip_special_tokens=True)\n",
        "print(res_square)\n",
        "print('\\n')\n",
        "llava_tri_out = llava.generate(**inputs_tri, max_new_tokens=100)\n",
        "res_tri = processor.decode(llava_tri_out[0], skip_special_tokens=True)\n",
        "print(res_tri)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display(img_square_small, img_tri_small)"
      ],
      "metadata": {
        "id": "_yjfhU4WR1Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conflict patching setup\n",
        "\n",
        "Emulate ideas from https://nnsight.net/notebooks/tutorials/attribution_patching/ but change the IOI setup to a conflict setup.\n",
        "\n",
        "Need multiple examples. Think carefully about constructing the clean and corrupted cases. Square basketball and Round basketball are both corrupted, and Round basketball is non-corrupted. For example...\n",
        "\n",
        "Need to also make sure I have the yes/no answers correctly setup...\n"
      ],
      "metadata": {
        "id": "2U5dwicXh0G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sanity check"
      ],
      "metadata": {
        "id": "9FOKQOvDAzpQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHi2ItA4BwfS"
      },
      "outputs": [],
      "source": [
        "prompt = f\"[INST] <image>\\nIs the object in the image a round basketball? Answer Yes or No.[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdX72VbX4UhD",
        "outputId": "120982bf-3fc2-429a-9f33-01bb39958484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes\n"
          ]
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)\n",
        "inputs = processor(images=img_square, text=prompt, return_tensors=\"pt\").to(device)\n",
        "with model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"]) as trace:\n",
        "    output = model.output.save()\n",
        "out_str = processor.decode(torch.argmax(output.logits[0,-1]), clean_up_tokenization_spaces=False)\n",
        "print(out_str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)\n",
        "inputs = processor(images=img_tri, text=prompt, return_tensors=\"pt\").to(device)\n",
        "with model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"]) as trace:\n",
        "    output = model.output.save()\n",
        "out_str = processor.decode(torch.argmax(output.logits[0,-1]), clean_up_tokenization_spaces=False)\n",
        "print(out_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZA6mv3nrG78",
        "outputId": "f7c5b6d0-b1d8-4132-d535-74d09b86786f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_answer_logits = (\"Yes\", \"No\")\n",
        "answer_token_ids = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.tokenize(compare_answer_logits))\n",
        "\n",
        "def answer_logits(inputs, answer_token_ids=answer_token_ids):\n",
        "  d = {}\n",
        "  with torch.no_grad():\n",
        "    answer_logits = model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"], trace=False).logits[0,-1, answer_token_ids].cpu()\n",
        "  for i, answer in enumerate(compare_answer_logits):\n",
        "    d[f\"predicted_logit_{answer}\"] = answer_logits[i].item()\n",
        "  return d"
      ],
      "metadata": {
        "id": "skguY052wWPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=img_square, text=prompt, return_tensors=\"pt\").to(device)\n",
        "res = answer_logits(inputs)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhd8Lr4qwk1t",
        "outputId": "c191f13f-dc27-4eca-d649-d5180b73c592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'predicted_logit_Yes': 19.59375, 'predicted_logit_No': 19.3125}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=img_tri, text=prompt, return_tensors=\"pt\").to(device)\n",
        "res = answer_logits(inputs)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWk0dcdSxh75",
        "outputId": "c7ff43d4-77e5-42c3-c84d-95e28ab20f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'predicted_logit_Yes': 19.1875, 'predicted_logit_No': 19.671875}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llava_out = llava.generate(**inputs, max_new_tokens=100)\n",
        "# res = processor.decode(llava_out[0], skip_special_tokens=True)\n",
        "# print(res)\n",
        "\n",
        "# inputs = processor(images=img_square, text=prompt, return_tensors=\"pt\").to(device)\n",
        "# square_logits = model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"], trace=False).logits.cpu()\n",
        "\n",
        "# inputs = processor(images=img_tri, text=prompt, return_tensors=\"pt\").to(device)\n",
        "# tri_logits = model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"], trace=False).logits.cpu()"
      ],
      "metadata": {
        "id": "XlUpWPVRqmVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple prompts\n",
        "\n",
        "Remember: text prompts must be the same len. Also, assuming image sizes must be the same as well."
      ],
      "metadata": {
        "id": "NDQcS-hHACxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# square basketball\n",
        "img_path = '/content/gdrive/MyDrive/vlm_mats_images/square_basketball.webp'\n",
        "img_square = Image.open(img_path)\n",
        "img_square = img_square.resize((1024, 1024))\n",
        "img_square_small = img_square.resize((300, 300))\n",
        "\n",
        "# round basketball\n",
        "img_path = '/content/gdrive/MyDrive/vlm_mats_images/round_basketball.jpeg'\n",
        "img_round = Image.open(img_path)\n",
        "img_round = img_round.resize((1024, 1024))\n",
        "img_round_small = img_round.resize((300, 300))"
      ],
      "metadata": {
        "id": "9B52QuebRkpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject = \"basketball\"\n",
        "expected_attribute = \"round\"\n",
        "unexpected_attribute = \"square\"\n",
        "\n",
        "clean_prompts = [\n",
        "  \"[INST] <image>\\nIs the {subject} in the image {attribute}? Answer Yes or No [/INST]\".format(\n",
        "      subject=subject, attribute=expected_attribute\n",
        "  ),\n",
        "  \"[INST] <image>\\nIs the {subject} in the image {attribute}? Answer Yes or No [/INST]\".format(\n",
        "      subject=subject, attribute=unexpected_attribute\n",
        "  ),\n",
        "  ]\n",
        "\n",
        "conflict_prompts = [\n",
        "  \"[INST] <image>\\nIs the {subject} in the image {attribute}? Answer Yes or No [/INST]\".format(\n",
        "      subject=subject, attribute=unexpected_attribute\n",
        "  ),\n",
        "  \"[INST] <image>\\nIs the {subject} in the image {attribute}? Answer Yes or No [/INST]\".format(\n",
        "      subject=subject, attribute=expected_attribute\n",
        "  ),\n",
        "  ]"
      ],
      "metadata": {
        "id": "Trwja4sQ9ZkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_prompts, conflict_prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut7m-Be4-LAf",
        "outputId": "633ffb7e-ee1a-450a-88e5-29d379120572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['[INST] <image>\\nIs the basketball in the image round? Answer Yes or No [/INST]',\n",
              "  '[INST] <image>\\nIs the basketball in the image square? Answer Yes or No [/INST]'],\n",
              " ['[INST] <image>\\nIs the basketball in the image square? Answer Yes or No [/INST]',\n",
              "  '[INST] <image>\\nIs the basketball in the image round? Answer Yes or No [/INST]'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using (300, 300) images to get things working with memory\n",
        "# 14.6GB -> 14.7GB\n",
        "torch.set_grad_enabled(False)\n",
        "clean_inputs = processor(images=[img_round_small, img_square_small], text=clean_prompts, return_tensors=\"pt\").to(device)\n",
        "conflict_inputs = processor(images=[img_round_small, img_square_small], text=conflict_prompts, return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "id": "SC48Ttes_bo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yes_no = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.tokenize((\"Yes\", \"No\")))\n",
        "no_yes = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.tokenize((\"No\", \"Yes\")))\n",
        "clean_answer_token_indices = torch.tensor([yes_no, yes_no])\n",
        "conflict_answer_token_indices = torch.tensor([no_yes, no_yes])\n",
        "print(clean_answer_token_indices), print(conflict_answer_token_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VxMXKzRsP6o",
        "outputId": "34d30baf-2e94-4123-8c76-2373ce964146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5592, 1770],\n",
            "        [5592, 1770]])\n",
            "tensor([[1770, 5592],\n",
            "        [1770, 5592]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14.7 -> 16.0\n",
        "torch.set_grad_enabled(False)\n",
        "clean_logits = model.trace(clean_inputs[\"input_ids\"], clean_inputs[\"pixel_values\"], clean_inputs[\"image_sizes\"], clean_inputs[\"attention_mask\"], trace=False).logits.cpu()\n",
        "conflict_logits = model.trace(conflict_inputs[\"input_ids\"], conflict_inputs[\"pixel_values\"], conflict_inputs[\"image_sizes\"], conflict_inputs[\"attention_mask\"], trace=False).logits.cpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ufQFE0-uqSb",
        "outputId": "6859edc1-6d4d-4af2-a656-cd4cac683100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logit_diff(logits, answer_token_indices=clean_answer_token_indices):\n",
        "    logits = logits[:, -1, :]\n",
        "    correct_logits = logits.gather(1, answer_token_indices[:,0].unsqueeze(1))\n",
        "    incorrect_logits = logits.gather(1, answer_token_indices[:,1].unsqueeze(1))\n",
        "    return (correct_logits - incorrect_logits).mean()"
      ],
      "metadata": {
        "id": "ckVAMkXRsczi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using (300, 300) -> clean logit diff: 0.46, Conflict logit diff: -1.2500\n",
        "# using (1024, 1024) -> Clean logit diff: 1.0078, Conflict logit diff: -0.8438\n",
        "\n",
        "\n",
        "CLEAN_BASELINE = get_logit_diff(clean_logits, clean_answer_token_indices).item()\n",
        "print(f\"Clean logit diff: {CLEAN_BASELINE:.4f}\")\n",
        "\n",
        "CONFLICT_BASELINE = get_logit_diff(conflict_logits, clean_answer_token_indices).item()\n",
        "print(f\"Conflict logit diff: {CONFLICT_BASELINE:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqWRFyUfpbl9",
        "outputId": "284d879e-5844-43a6-ffae-5f85e70f62ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean logit diff: 0.4609\n",
            "Conflict logit diff: -1.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attribution patching over components\n",
        "\n",
        "Check that clean baseline is 1.0 and conflict baseline is 0.0"
      ],
      "metadata": {
        "id": "H8DtuDDzyLQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conflict_metric(\n",
        "    logits,\n",
        "    answer_token_indices=clean_answer_token_indices,\n",
        "):\n",
        "    return (get_logit_diff(logits, answer_token_indices) - CONFLICT_BASELINE) / (\n",
        "        CLEAN_BASELINE - CONFLICT_BASELINE\n",
        "    )\n",
        "\n",
        "print(f\"Clean Baseline is 1: {conflict_metric(clean_logits).item():.4f}\")\n",
        "print(f\"Conflict Baseline is 0: {conflict_metric(conflict_logits).item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c0khbYyzFV_",
        "outputId": "b6041e95-e554-476e-ad0f-096ed431c18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Baseline is 1: 1.0000\n",
            "Conflict Baseline is 0: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### accessing internal components\n",
        "\n",
        "print(model)\n",
        "\n",
        "Then access like:\n",
        "\n",
        "\n",
        "*   model.vision_model\n",
        "*   model.language_model\n",
        "*   model.language_model.model.embed_tokens\n",
        "\n",
        "etc\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kziRNZwzI-rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_grad_enabled(False)\n",
        "# inputs = processor(images=img_tri, text=prompt, return_tensors=\"pt\").to(device)\n",
        "# with model.trace(inputs[\"input_ids\"], inputs[\"pixel_values\"], inputs[\"image_sizes\"], inputs[\"attention_mask\"]) as trace:\n",
        "#     logits = model.output.logits.save()\n",
        "# out_str = processor.decode(torch.argmax(logits[0,-1]), clean_up_tokenization_spaces=False)\n",
        "# print(out_str)"
      ],
      "metadata": {
        "id": "na0m8uwWsS80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OutOfMemoryError  \n",
        "\n",
        "40GB GPU RAM not enough using A100..."
      ],
      "metadata": {
        "id": "KtzOBgBOLoQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try just one layer\n",
        "for ix, layer in enumerate(model.language_model.model.layers[:1]):\n",
        "  print(ix)\n",
        "  print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skLQp8b-Qu3m",
        "outputId": "a04a9883-db80-4c16-d33d-b0403e456130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "MistralDecoderLayer(\n",
            "  (self_attn): MistralSdpaAttention(\n",
            "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (mlp): MistralMLP(\n",
            "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "    (act_fn): SiLU()\n",
            "  )\n",
            "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc;"
      ],
      "metadata": {
        "id": "VsqcmILWqFIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trying just one layer. 16.0 -> 40 GB immediately hmm\n",
        "# loading model consumes ~15GB\n",
        "# processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", cache_dir=\"./checkpoints\")\n",
        "# llava = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "#                                                           cache_dir=\"./checkpoints\",\n",
        "#                                                           torch_dtype=torch.float16,\n",
        "#                                                           low_cpu_mem_usage=True)\n",
        "# llava.to(device)\n",
        "# model = nnsight.NNsight(llava)\n",
        "#\n",
        "# image_size = (300, 300)\n",
        "# prompt = '[INST] <image>\\nIs the basketball in the image round? Answer Yes or No [/INST]'\n",
        "# clean_inputs = processor(images=[img_round_small, img_square_small], text=clean_prompts, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "clean_out = []\n",
        "conflict_out = []\n",
        "conflict_grads = []\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "with model.trace() as tracer:\n",
        "\n",
        "    with tracer.invoke(clean_inputs[\"input_ids\"], clean_inputs[\"pixel_values\"], clean_inputs[\"image_sizes\"], clean_inputs[\"attention_mask\"]) as invoker_clean:\n",
        "\n",
        "        #for layer in model.language_model.model.layers:\n",
        "        for layer in model.language_model.model.layers[:1]:\n",
        "            torch.set_grad_enabled(True)\n",
        "            attn_out = layer.self_attn.o_proj.input\n",
        "            clean_out.append(attn_out.save())\n",
        "            torch.set_grad_enabled(False)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "    with tracer.invoke(conflict_inputs[\"input_ids\"], conflict_inputs[\"pixel_values\"], conflict_inputs[\"image_sizes\"], conflict_inputs[\"attention_mask\"]) as invoker_corrupted:\n",
        "\n",
        "        # for layer in model.language_model.model.layers:\n",
        "        for layer in model.language_model.model.layers[:1]:\n",
        "            torch.set_grad_enabled(True)\n",
        "            attn_out = layer.self_attn.o_proj.input\n",
        "            conflict_out.append(attn_out.save())\n",
        "            # conflict_grads.append(attn_out.grad.save()) # this is where memory blows up\n",
        "            attn_out_grad = attn_out.grad\n",
        "            conflict_grads.append(attn_out_grad.save())\n",
        "            torch.set_grad_enabled(False)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        logits = model.output.logits.save()\n",
        "        # Our metric uses tensors saved on cpu, so we\n",
        "        # need to move the logits to cpu.\n",
        "        value = conflict_metric(logits.cpu())\n",
        "        value.backward()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.set_grad_enabled(False) # setting this to True blows up memory, setting to false errors with `RuntimeError: cannot register a hook on a tensor that doesn't require gradient`"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UgtQiQxWyNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh0joP3GihAV",
        "outputId": "1c7693ae-dbbc-493b-920a-6f95bf062184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "model = NNsight(llava)"
      ],
      "metadata": {
        "id": "vutu5W63il-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8YVFzPiir1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_grad_enabled(False)\n",
        "# with model.trace() as tracer:\n",
        "\n",
        "#     with tracer.invoke(clean_inputs[\"input_ids\"], clean_inputs[\"pixel_values\"], clean_inputs[\"image_sizes\"], clean_inputs[\"attention_mask\"]):\n",
        "#         print(\"invoker_clean\")\n",
        "#         # out1 = tracer.output.save()\n",
        "\n",
        "#     with tracer.invoke(conflict_inputs[\"input_ids\"], conflict_inputs[\"pixel_values\"], conflict_inputs[\"image_sizes\"], conflict_inputs[\"attention_mask\"]):\n",
        "#         print(\"invoker_corrupted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M34GiqDM7hVr",
        "outputId": "765d5980-7580-470a-bf59-fe7201e15138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "invoker_clean\n",
            "invoker_corrupted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patching_results = []\n",
        "\n",
        "for conflict_grad, conflict, clean, layer in zip(\n",
        "    conflict_grads, conflict_out, clean_out, range(len(clean_out))\n",
        "):\n",
        "\n",
        "    residual_attr = einops.reduce(\n",
        "        conflict_grad.value[:,-1,:] * (clean.value[:,-1,:] - conflict.value[:,-1,:]),\n",
        "        \"batch (head dim) -> head\",\n",
        "        \"sum\",\n",
        "        head = 12,\n",
        "        dim = 64,\n",
        "    )\n",
        "\n",
        "    patching_results.append(\n",
        "        residual_attr.detach().cpu().numpy()\n",
        "    )"
      ],
      "metadata": {
        "id": "VjC0h9w9H5g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.imshow(\n",
        "    patching_results,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0,\n",
        "    title=\"Patching Over Attention Heads\"\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Head\",\n",
        "    yaxis_title=\"Layer\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NURkKj5EIIh_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMcEwAl5JsHvozvcD+TFBpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}